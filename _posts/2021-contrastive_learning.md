---
layout: post
title: "Ideas in self-supervised contrastive learning."
tags: [Linux,Python,mpv]
---

<ul id="toc"></ul>

---
## The training dynamics / convergence analysis of the features or weights of contrastively trained models.
- Can we use Kuramoto model to model this part? To be specific, a novel reverse-Kuramoto model that looks at the de-synchronization of features.

## Analyzing the effect of batch normalization.
- Minimization of in-group difference.
- Batch normalization similar to the "advtange" used in reinforcement learning.
- Batch normalization (batch mean subtraction and batch norm devision) serves as a Expectation miximization method.
