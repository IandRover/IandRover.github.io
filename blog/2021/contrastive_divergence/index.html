<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Contrastive_divergence | Chia-Hsiang Kao</title> <meta name="author" content="Chia-Hsiang Kao"> <meta name="description" content="An exploration into the two perspectives of the Contrastive Divergence Algorithm."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iandrover.github.io/blog/2021/contrastive_divergence/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Chia-HsiangÂ </span>Kao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/"></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Contrastive_divergence</h1> <p class="post-meta">October 17, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a> Â  Â· Â  <a href="/blog/tag/research"> <i class="fas fa-hashtag fa-sm"></i> Research</a> Â  Â  Â· Â  <a href="/blog/category/articles"> <i class="fas fa-tag fa-sm"></i> articles</a> Â  </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="on-two-perspectives-of-contrastive-divergence-algorithm">On Two Perspectives of Contrastive Divergence Algorithm</h1> <h2 id="introduction">Introduction</h2> <p>Contrastive Divergence (CD) was proposed by Hinton and has been shown as a powerful algorithm to update the Restricted Boltzmann Machine (RBM).</p> <p>The objective of RBM is to best reconstruct the input data from a dataset $V$ and at the same time to find a good representation of the input data from $V$.</p> <h3 id="the-structure-of-rbm">The Structure of RBM</h3> <p>Basically, a RBM contains two layers: visible layer (with visible nodes) and hidden layer (with hidden nodes). RBM is parameterized with a weight matrix $W$ and two biases $a$ and $b$.</p> <ul> <li><img src="https://miro.medium.com/max/700/1*LoeBW9Stm6HjK57yBp45sQ.png" alt=""></li> </ul> <p>Consider now we have a set of patterns to be remembered, for example 60k MNIST images. The inference procedure is as follows:</p> <ul> <li>Given input $v_0$ from MNIST.</li> <li>Obtain the 0-1 encoded hidden representation $h_0$: $p(â„_0=1â”‚ğ‘£_0)=ğœ(ğ‘+ğ‘Š^ğ‘‡ ğ‘£_0)$</li> <li>Obtain the 0-1 encoded reconstruction $v_1$: $p(v_1=1â”‚h_0)=ğœ(a+ğ‘Š h_0)$</li> </ul> <p>The objective seems simple, but how can we effortless find a good ${W, a, b}$ to achieve so, especially without the gradient descent?</p> <h3 id="the-objective-of-rbm">The Objective of RBM</h3> <p>Let us formally define the objective of RBM.</p> <ul> <li>The energy of RBM given $v$ and $h$: $ğ¸(ğ‘£,â„)=âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’ğ‘^ğ‘‡ â„âˆ’ğ‘£^ğ‘‡ ğ‘Šâ„$. <ul> <li>Please note that since $h_0$ is sampled from $ğœ(ğ‘+ğ‘Š^ğ‘‡ ğ‘£_1)$, $h_0$ is stochastic.</li> </ul> </li> <li>The likelihood of $v_0$: $P(v_0) = \frac{1}{ğ‘} âˆ‘<em>â„ ğ‘’xp^{âˆ’ğ¸(ğ‘£_0,h)}$ , where $Z=âˆ‘</em>{ğ‘£,â„} ğ‘’xp^{âˆ’ğ¸(ğ‘£,â„)}$ <ul> <li>Note that the definition of $P(v_0)$ consider all possible 0-1 coded $h$, including $h=[0,â€¦,0]$ or $h=[1,â€¦,1]$, which is exponentially many.</li> <li>Note that the definition of $Z$ requires considering all possible 0-1 coded $v$ and $h$ pairs, which is also exponentially many.</li> </ul> </li> <li>The objective of RBM is to maximize the likelihood of all $v_0$ from a dataset $V$: $J(W,a,b)=argmax_{{ğ‘Š,a,b}}â¡âˆ‘_{ğ‘£_0âˆˆğ‘‰} logâ¡(P(ğ‘£_0))$. <ul> <li>Also, we have $J(W,a,b)=argmax_{{ğ‘Š,a,b}}â¡âˆ‘_{ğ‘£_0âˆˆğ‘‰} logâ¡(âˆ‘_â„ ğ‘’xp^{âˆ’ğ¸(ğ‘£_0,h)})-log(Z)$</li> <li>The illustration of the objective of RBM is provided here:</li> <li><img src="https://i.imgur.com/Ib1x2Ep.png" alt=""></li> </ul> </li> </ul> <p>Clearly, the objective of RBM is not only to best maximize the energy of the seen data but to minimize the energy of the unseen data. For example, we expect that a RBM trained on MNIST to learn to specifically encode the digits, but not be able to encode some white noises.</p> <h3 id="the-derivation-of-contrastive-divergence">The Derivation of Contrastive Divergence</h3> <p>To minimize $Z$ is not tractable. Hinton et al. proposed Contrastive Divergence (CD) to solve the problem. And since the birth of CD, there have been many modifications of the original CD method. With CD-k, we update the parameters $W$ by:</p> <ul> <li>$\Delta W=v_0\cdot h_0^\top-v_{k}\cdot h_{k}^\top$, where $v_k$ and $h_k$ is generated by running the sampling steps as illustrated below:</li> <li><img src="https://www.researchgate.net/profile/Baptiste-Wicht/publication/307908790/figure/fig1/AS:404331623927809@1473411579320/Graphical-representation-of-the-Contrastive-Divergence-Algorithm-The-algorithm-CD-k.png" alt=""></li> </ul> <p>CD has its distributional meaning defined using the K-L divergence. Please refer to Section 2.1 of paper <a href="https://openreview.net/pdf?id=MLSvqIHRidA" rel="external nofollow noopener" target="_blank">Contrastive Divergence Learning is a Time Reversal Adversarial Game (ICLR 2021)</a>.</p> <p>In <a href="http://proceedings.mlr.press/v9/sutskever10a/sutskever10a.pdf" rel="external nofollow noopener" target="_blank">On the Convergence Properties of Contrastive Divergence</a>, Ilya Sutskever et all. showed that CD is not a gradient of any function due to some inconsistency. And some other papers also show that CD has multiple local minimums and therefore can impede learning. There have been some papers working to better understand the mechanism of CD, for example <a href="https://openreview.net/pdf?id=MLSvqIHRidA" rel="external nofollow noopener" target="_blank">Contrastive Divergence Learning is a Time Reversal Adversarial Game (ICLR 2021)</a>. I think there can be other explanations.</p> <p>In this post, we want to propose two interesting viewpoints, though both of them are immature and incomplete.</p> <h2 id="understanding-contrastive-divergence-1">Understanding Contrastive Divergence (1).</h2> <p>Now we provide the more complete idea of the two. Before we start, please allow us to first review the logistic regression.</p> <h3 id="a-review-of-logistic-regression">A Review of Logistic Regression</h3> <p>Consider input ${x_1, x_2, â€¦, x_n}$ and one-hot target ${y_1, y_2, â€¦, y_n}$, we perform classification using a model parameterized by $\theta$: \(\begin{aligned} f_\theta(x)=\sigma(\theta^T x) \end{aligned}\) where $\sigma$ once again is the sigmoid function. The log likelihood if given by \(\begin{aligned} L(\theta)=\sum_{i=1}^n y_i log(\sigma(\theta^T x_i)) + (1-y_i) log(1-\sigma(\theta^T x_i)) \end{aligned}\) The gradient of the log likelihood is the cross product between the error and the input. \(\begin{aligned} \frac{\partial L(\theta)}{\partial \theta} =\sum_{i=1}^n (y_i-\sigma(\theta^T x_i))x_i^T \end{aligned}\) Below, we show that the components of the CD update of $W$ takes the form of the gradient in the Logistic regression.</p> <h3 id="a-k-step-contrastive-divergence-contains-2k-logistic-regression-tasks">A k-step Contrastive Divergence Contains 2*k Logistic Regression Tasks.</h3> <p>Looking at the CD update of $W$, we observe that \(\begin{aligned} \Delta W &amp; =v_0\cdot h_0^\top-v_{k}\cdot h_{k}^\top \\ &amp; =(v_0-v_1)\cdot h_0^\top + v_1\cdot (h_0-h_1) + ... + v_k\cdot (h_{k-1}-h_k)^\top \\ \end{aligned}\) For each component $(v_0-v_1)\cdot h_0^\top$, we find that it has the same form of the gradient used in Logistic Regression as we just reviewed. That means for each term $(v_0-v_1)\cdot h_0^\top$, the corresponding loss corresponds to a task of the following setting:</p> <ul> <li>$v_0$: the one-hot target</li> <li>$h_0$: the input</li> <li>$v_1$: the output</li> <li>$L(W)=v_0 log(v_1) + (1-v_0) log(1-v_1)$, where $v_1=\sigma(a+Wh_0)$</li> </ul> <p>Here we provide the illustration: <img src="https://i.imgur.com/6LQzNkb.png" alt=""></p> <p>To wrap up, the CD algorithm can be considered as iteratively performing logistic regression tasks. The intuition behind is by decomposing the CD update where we can see that each component actually follows the update used in logistic regression. By the way, the task itself can also be considered as a reconstruction of the target.</p> <h2 id="understanding-contrastive-divergence-2">Understanding Contrastive Divergence (2).</h2> <p>Here we provide another point of view which is more incomplete and immature. Nevertheless, it is still worth proving our intuition.</p> <h3 id="the-energy-function-in-rbm-is-an-activation-function">The Energy Function in RBM is an Activation Function.</h3> <p>We can rewrite the the energy function of RBM given $v$ and $h$: \(\begin{aligned} ğ¸(ğ‘£,â„) &amp; =âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’ğ‘^ğ‘‡ â„âˆ’ğ‘£^ğ‘‡ ğ‘Šâ„ \\ &amp; =âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’(ğ‘ +ğ‘Š^T v)^\top â„ \\ &amp; \sim âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’(ğ‘ +ğ‘Š^T v)^\top \sigma(ğ‘ +ğ‘Š^T v) \\ &amp; = âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’sum(SiLU(ğ‘ +ğ‘Š^T v))\\ \end{aligned}\) where SiLU stands for Sigmoid Linear Unit and is $SiLU(x)=x\cdot\sigma(x)$</p> <p>Till here, we have an intuition that the objective of RBM is like maximizing the SiLU activation of $ğ‘ +ğ‘Š^T v$ when $v$ comes from dataset, and suppressing the SiLU activation of other inputs.</p> <p>The derivative of SiLU is not easy to cope with for further analysis. We can here consider an asymptotic case of sigmoid: \(\begin{aligned} \sigma_\alpha(x) = \frac{1}{1+exp(-\alpha x)} \end{aligned}\) It is clear that when $\alpha$ goes to infinity, sigmoid function becomes a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function" rel="external nofollow noopener" target="_blank">step function</a> \(\begin{aligned} step(x) = 1_{\{x&gt;0\}} \end{aligned}\) Also, when $\alpha$ goes to infinity, SiLU function becomes a ReLU function.</p> <p>One property of this step function is that inferencing the hidden representation is no more stochastic: $h_0$: $p(â„_0=1â”‚ğ‘£_0)=step(ğ‘+ğ‘Š^ğ‘‡ ğ‘£_0)$. Also, the step function is the derivative of the ReLU function, meaning that we can get a modified energy function ğ¸(ğ‘£) (which depends on h): \(\begin{aligned} ğ¸(ğ‘£) &amp; =âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’(ğ‘ +ğ‘Š^T v)^\top step(ğ‘ +ğ‘Š^T v) \\ &amp; =âˆ’ğ‘^ğ‘‡ ğ‘£âˆ’sum(ReLU(ğ‘ +ğ‘Š^T v))\\ \end{aligned}\)</p> <h2 id="the-reconstruction-of-v_0-is-the-gradient-of-energy-function-wrt-v_0-">The Reconstruction of $v_0$ is the Gradient of Energy Function w.r.t. $v_0$ .</h2> <p>Now something cool happens. We can calculate the derivative of the energy function $ğ¸(ğ‘£_0)$ w.r.t the input $v_0$. \(\begin{aligned} \frac{\partial E(v_0)}{\partial v_0} &amp; = - \frac{\partial ğ‘^ğ‘‡ ğ‘£_0+sum(ReLU(ğ‘ +ğ‘Š^T v_0))}{\partial v_0} \\ &amp; = - (a + W step(b+W^\top v_0)) \\ &amp; = - (a + W h_0) \end{aligned}\) Surprisingly, we can discover that it is the preactivation of the reconstruction $v_1$. This means that we can express $v_1$ in two manners: \(\begin{aligned} &amp; v_1 = step(a+W h_0)\\ &amp; v_1 = step(- \frac{\partial E(v_0)}{\partial v_0} )\\ \end{aligned}\)</p> <p>What does this tell us? Recall that we are to minimize the energy (in order to maximize the likelihood), the negative gradient of the energy function (i.e. $- \frac{\partial E(v_0)}{\partial v_0}$) actually entails that direction to lower energy (theoretically). The step function helps normalize the range into 0-1. Also note that, if the value of the gradient is large enough, we can approximately say that: \(\begin{aligned} &amp; v_1 = step(v_0 - \frac{\partial E(v_0)}{\partial v_0} ) \end{aligned}\) , when the norm of $v_0$ is much smaller than the norm of $\frac{\partial E(v_0)}{\partial v_0}$.</p> <p>But now, I cannot combine the concept that â€œthe reconstruction is the gradientâ€ to the CD update of $W$. Because I do not know how to explain the update from this point of view.</p> <h2 id="final-words">Final Words</h2> <p>Thank you for taking your time. Generally speaking, I love the mysterious CD algorithm and I still think there can be some other explanation to the CD algorithm. The two points of view provided above are both simply and straight-forward. Hope that this post can give you some inspirations.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/media/">Media</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Chia-Hsiang Kao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>