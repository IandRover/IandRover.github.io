<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  </style>
  <link rel="icon" type="image/png" href="images/great_wave_thunmnail.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Yen-Chi Cheng</title>
  <meta name="Yen-Chi Cheng's Homepage" http-equiv="Content-Type" content="Yen-Chi Cheng's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Yen-Chi Cheng</font><br> -->
    <pageheading>Yen-Chi Cheng</pageheading><br>
    <!-- <b>email</b>: charlescheng0117_at_gmail_dot_com -->
    <!-- <b>email</b>: yenchicheng_at_cmu.edu -->
    <!-- <b>email</b>: yenchich_at_andrew.cmu.edu -->
    <b>email</b>: yenchich_at_cs.cmu.edu
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <!-- <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', '1cm0asg17eccmilnrlah.oge@h',
        [14, 8, 19, 13, 20, 7, 12, 15, 16, 6, 1, 24, 26, 21, 5, 11, 4, 22, 3, 9, 23, 25, 18, 10, 17, 2]);
        // 'emailScramble', 'charlescheng0117@gmail.com',
        // [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]);
    </script> -->
  </p>

  <tr>
    <!-- <td width="32%" valign="top"><a href="images/yenchicheng.png"><img src="images/yenchicheng.png" width="100%" style="border-radius:15px"></a> -->
    <td width="32%" valign="top"><a href="images/image_20210220.png"><img src="images/image_20210220.png" width="100%" style="border-radius:15px"></a>
        <p align=center>
            | <a href="https://drive.google.com/open?id=1W6Paq4gn2X3wAQIaqWiqqpDJgcy6noA5">CV</a> |
            <a href="https://scholar.google.cl/citations?hl=en&pli=1&user=wvuEiWgAAAAJ">Google Scholar</a> |<br/>|
            <a href="https://github.com/yccyenchicheng">Github</a> |
            <a href="https://www.linkedin.com/in/yen-chi-cheng-464457b5/">LinkedIn</a> |
        </p>
    </td>

    <td width="68%" valign="top" align="justify">
        <p>
        I am a MS student at the RI, CMU School of Computer Science and a research intern at 
        <a href="https://research.snap.com/team/category/creative-vision/">Snap Research</a> working with 
        <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a> and <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.
        Previously, I was lucky to have the opportunities to work with 
        <a href="http://www.cs.nthu.edu.tw/~lai/"> Prof. Shang-Hong Lai</a> as a research intern at Microsoft,
        <a href="https://faculty.ucmerced.edu/mhyang/">Prof. Ming-Hsuan Yang</a> as a visiting scholar at UC Merced,
        <a href="https://aliensunmin.github.io/">Prof. Min Sun</a> and <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a>
        as a research assistant at National Tsing Hua University.
        My research interests include Computer Vision and Deep Learning.


        <!-- I am an incoming graduate student at CMU RI. Currently, I am a research intern at Microsoft AI R&D Center, Taiwan 
        supervised by <a href="http://www.cs.nthu.edu.tw/~lai/"> Prof. Shang-Hong Lai</a>. 
        Previously, I was lucky to have the opportunities to work with <a href="https://faculty.ucmerced.edu/mhyang/">Prof. Ming-Hsuan Yang</a>
        as a visiting scholar at UC Merced, <a href="https://aliensunmin.github.io/">Prof. Min Sun</a> and
        <a href="https://htchen.github.io/">Prof. Hwann-Tzong Chen</a> as a research assistant at National Tsing Hua University.
        My research interests include Computer Vision and Deep Learning. -->

        <!-- I am a visiting scholar at UC Merced advised by <a href="https://faculty.ucmerced.edu/mhyang/">Prof. Ming-Hsuan Yang</a>. 
        Previously, I was lucky to have the opportunity to work with Prof. <a href="https://aliensunmin.github.io/">Min Sun</a> and 
        Prof. <a href="https://htchen.github.io/">Hwann-Tzong Chen</a> at National Tsing Hua University.
        My research interests include Computer Vision and Deep Learning.  -->

        <br>
        <br>

        I decided to switch field from Economics/Finance to Computer Science due to my research experiences in this field. I would like to pursue a 
        Ph.D. degree in Computer Science to fulfill my career goal.

        <!-- I decided to switch field from Economics/Finance to Computer Science due to my research experiences in this field. Taking several CS and ML courses
        at UC Berkeley and NTU piqued my interest in this field. My research experiences made me determine to conduct advanced studies in CS.
        I would like to pursue a Ph.D./M.S. degree in Computer Science to fulfill my career goal. -->

        <!-- I decided to switch field from Economics/Finance to Computer Science due to my past experiences in this field. Taking several CS and ML courses
        at UC Berkeley and NTU piqued my interest in CS. And my research experiences as a research assistant in <a href="https://aliensunmin.github.io/lab/info.html">VSLab</a>
        at NTHU and an intern at an AI startup, <a href="https://www.relajet.com/">RelaJet</a>, made me determine to conduct advanced studies in CS.
        I would like to pursue a Ph.D./M.S. degree in Computer Science to fulfill my career goal. -->
        <br>
        <br>
        Please see my <a href="https://drive.google.com/open?id=1W6Paq4gn2X3wAQIaqWiqqpDJgcy6noA5">CV</a> (last updated May 21) for more details.
        <br>
        <br>
        <!-- <b>I am applying for M.S. in Computer Science for 2020 Fall.</b> -->
        </p>

        </td>
  </tr>
</table>


<!-- =================== Experience =================== -->
<table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
    <tr>
        <th width="16.7%" valign="top" align="center">
        <img src="images/snap_logo.png" alt="sym" width="60%"></a>
        <!-- <p style="line-height:1.3; font-size:12pt">CMU<br>M.S. in Computer Vision<br>Jan. 21 - June. 22</p> -->
        <p style="line-height:1.3; font-size:12pt">Snap Inc.<br>Research Intern<br>May. 21 - Present</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/cmu.png" alt="sym" width="90%"></a>
        <!-- <p style="line-height:1.3; font-size:12pt">CMU<br>M.S. in Computer Vision<br>Jan. 21 - June. 22</p> -->
        <p style="line-height:1.3; font-size:12pt">CMU<br>MSCV<br>Jan. 21 - June. 22</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/microsoft.png" alt="sym" width="80%"></a>
        <p style="line-height:1.3; font-size:12pt">Microsoft<br>Research Intern<br>Mar. 20 - Present</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/ucm.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">UC Merced<br>Visiting Scholar<br>Sept. 19 - Mar. 20</p>
        </th>

        <th width="16.6%" valign="top" align="center">
        <img src="images/nthu.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">NTHU<br>Research Assistant<br>Sept. 18 - Feb. 20</p>
        </th>

        <!-- <th width="16.6%" valign="top" align="center">
        <img src="images/relajet.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">RelaJet<br>Intern<br>March 18 - Aug. 18</p>
        </th> -->

        <th width="16.6%" valign="top" align="center">
        <img src="images/ntu.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">NTU<br>
          M.B.A. in Finance<br>
          (Leave of Absence)<br>
          Sept. 17 - June 18</p>
        </th>

        <!-- <th width="16.6%" valign="top" align="center">
        <img src="images/ucb.png" alt="sym" width="60%"></a>
        <p style="line-height:1.3; font-size:12pt">UC Berkeley<br>Summer Session<br>June. 17 - Aug. 17</p>
        </th> -->
    </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
      <li> [05/2021] Start my research internship at <a href="https://research.snap.com/team/category/creative-vision/">Snap Research</a> working with 
        <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a> and <a href="http://www.stulyakov.com/">Sergey Tulyakov</a>.</li>
      <li> [02/2021] Start my graduate study at <b>CMU RI</b>!</li>
      <li> [07/2020] One paper accepted at <b>ECCV'20</b>.</li>
      <li> [03/2020] Start my research internship at Microsoft AI R&D Center, Taiwan working with
          <a href="http://www.cs.nthu.edu.tw/~lai/"> Prof. Shang-Hong Lai</a>.</li>
      <li> [09/2019] Start my visiting in <a href="http://vllab.ucmerced.edu/">VLLab</a> at UC Merced working with 
          <a href="https://faculty.ucmerced.edu/mhyang/">Prof. Ming-Hsuan Yang</a>.</li>
      <li> [07/2019] One paper accepted at <b>ICCV'19</b>. See you at Seoul!</li>
      <li> [11/2018] One workpaper accepted at <b>NeurIPS'18 (spotlight)</b>.</li>
      <li> [09/2018] Start working as a research assistant at NTHU advised by <a href="https://aliensunmin.github.io/">Prof. Min Sun</a>.</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="images/inout_thumbnail.png" alt="sym" width="300" height="200" style="border-radius:15px">
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="#" id="InOut21">

            <heading>In&Out: Diverse Image Outpainting via GAN Inversion</heading></a>
            <br>
            <u><b>Yen-Chi Cheng</b></u>, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang
            <br>
            <!-- Under review -->
            arXiv 2021
        </p>

        <div class="paper" id="inout21">
        <a href="https://yccyenchicheng.github.io/InOut/">webpage </a> |
        <a href="javascript:toggleblock('inout21_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('inout21')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/abs/2104.00675">arXiv</a> |
        <a href="https://github.com/yccyenchicheng/InOut">code (coming soon)</a>

        <p align="justify">
            <i id="inout21_abs">
              Image outpainting seeks for a semantically consistent extension of the input image beyond its available content.
              Compared to inpainting -- filling in missing pixels in a way coherent with the neighboring pixels -- outpainting
              can be achieved in more diverse ways since the problem is less constrained by the surrounding pixels.
              Existing image outpainting methods pose the problem as a conditional image-to-image translation task,
              often generating repetitive structures and textures by replicating the content available in the input image.
              In this work, we formulate the problem from the perspective of inverting generative adversarial networks.
              Our generator renders micro-patches conditioned on their joint latent code as well as their individual positions in the image.
              To outpaint an image, we seek for multiple latent codes not only recovering available patches but also
              synthesizing diverse outpainting by patch-based generation. This leads to richer structure and content in the outpainted regions.
              Furthermore, our formulation allows for outpainting conditioned on the categorical input, thereby enabling flexible user controls.
              Extensive experimental results demonstrate the proposed method performs favorably against existing in- and outpainting methods,
              featuring higher visual quality and diversity.
            </i>
        </p>

        <pre xml:space="preserve">
        @article{cheng2021inout,
            author = {
                    Cheng, Yen-Chi and 
                    Lin, Chieh Hubert and
                    Le, Hsin-Ying and 
                    Ren, Jian and
                    Tulyakov, Sergey and
                    Yang, Ming-Hsuan
                  },
            title = {{In&Out}: Diverse Image Outpainting via GAN Inversion},
            journal={arXiv preprint arXiv:2104.00675},
            year = {2021}
            }
        </pre>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="images/infinityGAN.png" alt="sym" width="300" height="200" style="border-radius:15px">
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="#" id="InOut21">

            <heading>InfinityGAN: Towards Infinite-Resolution Image Synthesis</heading></a>
            <br>
            Chieh Hubert Lin, Hsin-Ying Lee, <u><b>Yen-Chi Cheng</b></u>, Sergey Tulyakov, Ming-Hsuan Yang
            <br>
            <!-- Under review -->
            arXiv 2021
        </p>

        <div class="paper" id="infinitygan21">
        <a href="https://hubert0527.github.io/infinityGAN/">webpage </a> |
        <a href="javascript:toggleblock('infinitygan21_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('infinitygan21')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/abs/2104.03963">arXiv</a> |
        <a href="https://github.com/hubert0527/infinityGAN">code (coming soon)</a>

        <p align="justify">
            <i id="infinitygan21_abs">
              Image outpainting seeks for a semantically consistent extension of the input image beyond its available content.
              Compared to inpainting -- filling in missing pixels in a way coherent with the neighboring pixels -- outpainting
              can be achieved in more diverse ways since the problem is less constrained by the surrounding pixels.
              Existing image outpainting methods pose the problem as a conditional image-to-image translation task,
              often generating repetitive structures and textures by replicating the content available in the input image.
              In this work, we formulate the problem from the perspective of inverting generative adversarial networks.
              Our generator renders micro-patches conditioned on their joint latent code as well as their individual positions in the image.
              To outpaint an image, we seek for multiple latent codes not only recovering available patches but also
              synthesizing diverse outpainting by patch-based generation. This leads to richer structure and content in the outpainted regions.
              Furthermore, our formulation allows for outpainting conditioned on the categorical input, thereby enabling flexible user controls.
              Extensive experimental results demonstrate the proposed method performs favorably against existing in- and outpainting methods,
              featuring higher visual quality and diversity.
            </i>
        </p>

        <pre xml:space="preserve">
        @article{lin2021infinity,
            author = {
                    Lin, Chieh Hubert and
                    Le, Hsin-Ying and 
                    Cheng, Yen-Chi and 
                    Tulyakov, Sergey and
                    Yang, Ming-Hsuan
                  },
            title = {{InfinityGAN}: Towards Infinite-Resolution Image Synthesis},
            journal={arXiv preprint arXiv:2104.03963},
            year = {2021}
            }
        </pre>

        </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <!-- <img src="images/coming_soon.jpg" alt="sym" width="70%" height="30%" style="border-radius:15px"> -->
        <!-- <img src="images/segvae_teaser.png" alt="sym" width="100%" height="15%" style="border-radius:15px"> -->
        <!-- <img src="images/segvae_teaser_v1.png" alt="sym" width="100%" height="100%" style="border-radius:15px"> -->
        <img src="images/segvae_teaser_v1.png" alt="sym" width="200" height="200" style="border-radius:15px">
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="#" id="SegVAE20">

            <heading>Controllable Image Synthesis via SegVAE</heading></a>
            <br>
            <u><b>Yen-Chi Cheng</b></u>, Hsin-Ying Lee, Min Sun, Ming-Hsuan Yang
            <br>
            <!-- Under review -->
            ECCV 2020
        </p>

        <div class="paper" id="segvae20">
        <a href="https://yccyenchicheng.github.io/SegVAE/">webpage </a> |
        <a href="javascript:toggleblock('segvae20_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('segvae20')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/abs/2007.08397">arXiv</a> |
        <a href="https://github.com/yccyenchicheng/SegVAE">code</a>

        <p align="justify">
            <i id="segvae20_abs">
              Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used 
	      intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, 
	      the semantic map enables simpler user modification. In this work, we specifically target at generating 
	      semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, 
	      synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative 
	      and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic
	      maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to 
	      better understand the quality of the synthesized semantic maps. Furthermore, we showcase several real-world
	      image-editing applications including object removal, object insertion, and object replacement.
            </i>
        </p>

        <pre xml:space="preserve">
        @inproceedings{cheng2020segvae,
            Author = {
            Cheng, Yen-Chi and 
            Lee, Hsin-Ying and
            Sun, Min and
            Yang, Ming-Hsuan
            },
            Title = {Controllable Image Synthesis via {SegVAE}},
            Booktitle = {ECCV},
            Year = {2020}
           }
        </pre>

        </div>
    </td>
  </tr>
 


  <tr>
      <td width="33%" valign="top" align="center">
          <a href="https://zswang666.github.io/P2PVG-Project-Page">
          <!-- <img src="images/iccv19.gif" alt="sym" width="100%" height="90%" style="border-radius:15px"></a> -->
          <img src="images/iccv19.gif" alt="sym" width="200" height="200" style="border-radius:15px"></a>
      </td>

      <td width="67%" valign="top">
          <p>
              <a href="https://zswang666.github.io/P2PVG-Project-Page" id="ICCV19">
              <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Point-to-Point Video Generation</heading></a>
              <br>
              <u><b>Yen-Chi Cheng*</b></u>, Tsun-Hsuan Wang*, Chieh Hubert Lin, Hwann-Tzong Chen, Min Sun
              <br>
              ICCV 2019
              <br>
              (* indicates equal contribution)
              <!-- ICCV 2019 , Seoul -->
          </p>
  
          <div class="paper" id="iccv19">
          <a href="https://zswang666.github.io/P2PVG-Project-Page/">webpage</a> |
          <a href="javascript:toggleblock('iccv19_abs')">abstract</a> |
          <a shape="rect" href="javascript:togglebib('iccv19')" class="togglebib">bibtex</a> |
          <a href="https://arxiv.org/abs/1904.02912">arXiv</a> |
          <a href="https://github.com/yccyenchicheng/p2pvg">code</a>
  
          <p align="justify">
              <i id="iccv19_abs">
                  While image synthesis achieves tremendous breakthroughs (e.g., generating realistic faces), video generation is 
                  less explored and harder to control, which limits its applications in the real world. For instance, video editing 
                  requires temporal coherence across multiple clips and thus poses both start and end constraints within a video sequence.
                  We introduce point-to-point video generation that controls the generation process with two control points: the targeted 
                  start- and end-frames. The task is challenging since the model not only generates a smooth transition of frames but also
                  plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of various lengths. 
                  We propose to maximize the modified variational lower bound of conditional data likelihood under a skip-frame training strategy. 
                  Our model can generate end-frame-consistent sequences without loss of quality and diversity. We evaluate our method through 
                  extensive experiments on Stochastic Moving MNIST, Weizmann Action, Human3.6M, and BAIR Robot Pushing under a series of scenarios. 
                  The qualitative results showcase the effectiveness and merits of point-to-point generation.
              </i>
          </p>
  
          <pre xml:space="preserve">
          @inproceedings{wang2019p2pvg,
              Author = {Wang, Tsun-Hsuan and 
                      Cheng, Yen-Chi and 
                      Lin, Chieh Hubert and 
                      Chen, Hwann-Tzong and 
                      Sun, Min},
              Title = {Point-to-Point Video Generation},
              Booktitle = {ICCV},
              Year = {2019}
              }
          </pre>

          </div>
      </td>
  </tr>

    <tr>
        <td width="33%" valign="top" align="center">
            <a href="#">
            <!-- <img src="images/neuripsw18.png" alt="sym" width="100%" height="15%" style="border-radius:15px"></a> -->
            <img src="images/neuripsw18.png" alt="sym" width="200" height="200" style="border-radius:15px"></a>
        </td>

        <td width="67%" valign="top">
            <p>
                <a href="https://arxiv.org/abs/1904.03086" id="NEURIPSW18">
                <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
                <heading>Radiotherapy Target Contouring with Convolutional Gated Graph Neural Network</heading></a>
                <br>
                Chun-Hung Chao, <u><b>Yen-Chi Cheng</b></u>, Hsien-Tzu Cheng, Chi-Wen Huang, Tsung-Ying Ho, Chen-Kan Tseng, Le Lu, Min Sun
                <br>
                NeurIPS 2018 Workshop <b>(Spotlight)</b>
                <br>
            </p>
    
            <div class="paper" id="neuripsw18">
            <!-- <a href="https://zswang666.github.io/P2PVG-Project-Page/">webpage</a> | -->
            <a href="javascript:toggleblock('neuripsw18_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('neuripsw18')" class="togglebib">bibtex</a> |
            <a href="https://arxiv.org/abs/1904.03086">arXiv</a>
            <!-- <a href="https://github.com/yccyenchicheng/p2pvg">code</a> -->
    
            <p align="justify">
                <i id='neuripsw18_abs'>
                    Tomography medical imaging is essential in the clinical workflow of modern cancer radiotherapy. Radiation oncologists identify 
                    cancerous tissues, applying delineation on treatment regions throughout all image slices. This kind of task is often formulated 
                    as a volumetric segmentation task by means of 3D convolutional networks with considerable computational cost. Instead, inspired 
                    by the treating methodology of considering meaningful information across slices, we used Gated Graph Neural Network to frame this 
                    problem more efficiently. More specifically, we propose convolutional recurrent Gated Graph Propagator (GGP) to propagate high-level 
                    information through image slices, with learnable adjacency weighted matrix. Furthermore, as physicians often investigate a few 
                    specific slices to refine their decision, we model this slice-wise interaction procedure to further improve our segmentation result.
                    This can be set by editing any slice effortlessly as updating predictions of other slices using GGP. To evaluate our method, we collect
                    an Esophageal Cancer Radiotherapy Target Treatment Contouring dataset of 81 patients which includes tomography images with radiotherapy 
                    target. On this dataset, our convolutional graph network produces state-of-the-art results and outperforms the baselines. With the addition 
                    of interactive setting, performance is improved even further. Our method has the potential to be easily applied to diverse kinds of medical
                    tasks with volumetric images. Incorporating both the ability to make a feasible prediction and to consider the human interactive input, 
                    the proposed method is suitable for clinical scenarios.
                </i>
            </p>
    
            <pre xml:space="preserve">
            @article{chao18radiotherapy,
                title     = {Radiotherapy Target Contouring with Convolutional Gated Graph Neural
                             Network},
                author    = {Chao, Chun-Hung and Cheng, Yen-Chi and Cheng, Hsien-Tzu and Huang, Chi-Wen and
                             Ho, Tsung-Ying and Tseng, Chen-Kan
                            Lu, Le and Sun, Min},
                journal   = {arXiv preprint arXiv:1904.02912},
                year      = {2019},
                }
            </pre>

            </div>
        </td>
    </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tr><td><sectionheading>&nbsp;&nbsp;Projects</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

    <tr>
        <td width="50%" valign="top" align="center">
            <a href="#">
            <!-- <img src="images/pytorch_videovae.png" alt="sym" width="100%" height="40%" style="border-radius:15px"></a> -->
            <img src="images/pytorch_videovae.png" alt="sym" width="350" height="200" style="border-radius:15px"></a>
        </td>

        <td width="50%" valign="top">
            <p>
                <a href="https://github.com/yccyenchicheng/pytorch-VideoVAE" id="PYTORCHVIDEOVAE">
                <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
                <heading>PyTorch VideoVAE</heading></a>
                <br>
                A PyTorch implementation of a video generation method with attribute control using VAE by 
                <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jiawei_He_Probabilistic_Video_Generation_ECCV_2018_paper.pdf" >He et al., ECCV 2018</a>.
                <br>
                | <a href="https://github.com/yccyenchicheng/pytorch-VideoVAE">code</a> |
            </p>

        </td>
    </tr>

    <tr>
        <td width="50%" valign="top" align="center">
            <a href="#">
            <!-- <img src="images/pytorch_seginpaint.png" alt="sym" width="100%" height="40%" style="border-radius:15px"></a> -->
            <img src="images/pytorch_seginpaint.png" alt="sym" width="350" height="200" style="border-radius:15px"></a>
        </td>

        <td width="50%" valign="top">
            <p>
                <a href="https://github.com/yccyenchicheng/pytorch-SegInpaint" id="PYTORCHSEGINPAINT">
                <!-- <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
                <heading>PyTorch SegInpaint</heading></a>
                <br>
                A PyTorch implementation of an inpainting method based on 
                <a href="https://arxiv.org/abs/1805.03356">Song et al., BMVC 2018</a>.
                <br>
                | <a href="https://github.com/yccyenchicheng/pytorch-SegInpaint">code</a> |
            </p>
    
        </td>
    </tr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
        <ul>
          <li> Reviewer: CVPR 2021 </li>
        </ul>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>
        <ul>
          <li> ICCV 2019 Travel Award </li>
        </ul>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="right"><font size="2">
    Template: <a href="https://people.eecs.berkeley.edu/~pathak/">this</a> and <a href="https://zswang666.github.io/">this</a>
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('inout21_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('infinitygan21_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('segvae20_abs');
</script>
  

<script xml:space="preserve" language="JavaScript">
hideblock('iccv19_abs');
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('neuripsw18_abs');
</script>

</script>
</body>

</html>
